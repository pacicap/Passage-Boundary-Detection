# Hyperparameters
LEARNING_RATE = 3e-5  # 1.5e-5, 2e-5, 3e-5, 5e-5
BATCH_SIZE = 16  # 8, 16, 32
WARMUP_STEPS = 1000  # 0, 100, 1000, 10000
NUM_EPOCHS = 10  # 3, 5, 10
WEIGHT_DECAY = 1e-3  # 1e-2 or 1e-3
DROP_OUT_RATE = 0.1  # 0.1 or 0.2


OUTPUT
===========

2023-12-19 19:20:57.012418: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-19 19:20:57.012481: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-19 19:20:57.014396: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-19 19:20:58.167038: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT

Using device: CUDA

Loading data from /content/sample_data/shared_data/dataset_2_2_pair_sentences_13b.jsonl...
Loaded 7043 items.

Class distribution:
- continue: 3548
- not_continue: 3495
continue (after trim) : 1774
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Training Epoch 1/10: 100% 264/264 [01:00<00:00,  4.33it/s]
Validating: 100% 33/33 [00:05<00:00,  6.40it/s]
* Training Loss: 0.7011, Validation Loss: 0.6700

Training Epoch 2/10: 100% 264/264 [01:01<00:00,  4.27it/s]
Validating: 100% 33/33 [00:05<00:00,  6.08it/s]
* Training Loss: 0.6770, Validation Loss: 0.6977

Training Epoch 3/10: 100% 264/264 [01:02<00:00,  4.20it/s]
Validating: 100% 33/33 [00:05<00:00,  5.81it/s]
* Training Loss: 0.6165, Validation Loss: 0.6214

Training Epoch 4/10: 100% 264/264 [01:03<00:00,  4.15it/s]
Validating: 100% 33/33 [00:05<00:00,  5.78it/s]
* Training Loss: 0.4877, Validation Loss: 0.6809

Training Epoch 5/10: 100% 264/264 [01:04<00:00,  4.11it/s]
Validating: 100% 33/33 [00:05<00:00,  5.66it/s]
* Training Loss: 0.2646, Validation Loss: 0.7958

Training Epoch 6/10: 100% 264/264 [01:04<00:00,  4.10it/s]
Validating: 100% 33/33 [00:05<00:00,  5.64it/s]
* Training Loss: 0.1319, Validation Loss: 1.0725

Training Epoch 7/10: 100% 264/264 [01:04<00:00,  4.10it/s]
Validating: 100% 33/33 [00:05<00:00,  5.60it/s]
* Training Loss: 0.0615, Validation Loss: 1.4227

Training Epoch 8/10: 100% 264/264 [01:04<00:00,  4.08it/s]
Validating: 100% 33/33 [00:05<00:00,  5.57it/s]
* Training Loss: 0.0376, Validation Loss: 1.4559

Training Epoch 9/10: 100% 264/264 [01:04<00:00,  4.07it/s]
Validating: 100% 33/33 [00:05<00:00,  5.54it/s]
* Training Loss: 0.0246, Validation Loss: 1.6509

Training Epoch 10/10: 100% 264/264 [01:04<00:00,  4.08it/s]
Validating: 100% 33/33 [00:05<00:00,  5.56it/s]
* Training Loss: 0.0189, Validation Loss: 1.5664

Accuracy: 0.6227703984819735
Training Class-wise metrics:
continue: Precision = 0.45, Recall = 0.58, F1 = 0.51
not_continue: Precision = 0.75, Recall = 0.65, F1 = 0.69
Testing: 100% 33/33 [00:06<00:00,  5.12it/s]
Average test loss: 1.473032478130225

Model: BERT

- Accuracy: 0.653
- Precision: 0.624
- Recall: 0.632
- F1 Score: 0.626
- AUC-ROC: 0.689
- Matthews Correlation Coefficient (MCC): 0.255
- Confusion Matrix:
              continue  not_continue
continue           101            77
not_continue       106           243

continue: Precision = 0.45, Recall = 0.58, F1 = 0.51
not_continue: Precision = 0.75, Recall = 0.65, F1 = 0.69

Hyperparameters:
- Learning Rate: 3e-05
- Batch Size: 16
- Warmup Steps: 1000
- Number of Epochs: 10
- Weight Decay: 0.001
- Dropout Rate: 0.1
---
- Seed: 42