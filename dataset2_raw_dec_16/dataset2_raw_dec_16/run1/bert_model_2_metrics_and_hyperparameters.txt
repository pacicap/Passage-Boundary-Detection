# Hyperparameters
LEARNING_RATE = 1.2e-5  # 1.5e-5, 2e-5, 3e-5, 5e-5
BATCH_SIZE = 16  # 8, 16, 32
WARMUP_STEPS = 1000  # 0, 100, 1000, 10000
NUM_EPOCHS = 3  # 3, 5, 10
WEIGHT_DECAY = 1e-3  # 1e-2 or 1e-3
DROP_OUT_RATE = 0.1  # 0.1 or 0.2


Using device: CUDA

Loading data from /content/sample_data/shared_data/dataset_2_2_pair_sentences.jsonl...
Loaded 7555 items.

Class distribution:
- continue: 3894
- not_continue: 3661
continue (after trim) : 1947
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading data from /content/sample_data/shared_data/dataset_2_2_pair_sentences.jsonl...
Loaded 7555 items.

Class distribution:
- continue: 3894
- not_continue: 3661
continue (after trim) : 1947

Training Epoch 1/3: 100% 561/561 [01:17<00:00,  7.24it/s]
Validating: 100% 71/71 [00:09<00:00,  7.70it/s]
* Training Loss: 0.6954, Validation Loss: 0.6338

Training Epoch 2/3: 100% 561/561 [01:17<00:00,  7.26it/s]
Validating: 100% 71/71 [00:09<00:00,  7.74it/s]
* Training Loss: 0.6416, Validation Loss: 0.7189

Training Epoch 3/3: 100% 561/561 [01:17<00:00,  7.26it/s]
Validating: 100% 71/71 [00:09<00:00,  7.71it/s]
* Training Loss: 0.5266, Validation Loss: 0.6782

Accuracy: 0.6387403446226976
Training Class-wise metrics:
continue: Precision = 0.48, Recall = 0.52, F1 = 0.50
not_continue: Precision = 0.73, Recall = 0.70, F1 = 0.72
Testing: 100% 71/71 [00:05<00:00, 11.91it/s]
Average test loss: 0.6815772514108202

Model: BERT

- Accuracy: 0.643
- Precision: 0.624
- Recall: 0.633
- F1 Score: 0.624
- AUC-ROC: 0.690
- Matthews Correlation Coefficient (MCC): 0.257
- Confusion Matrix:
              continue  not_continue
continue           117            78
not_continue       122           244

continue: Precision = 0.48, Recall = 0.52, F1 = 0.50
not_continue: Precision = 0.73, Recall = 0.70, F1 = 0.72

Hyperparameters:
- Learning Rate: 2e-05
- Batch Size: 8
- Warmup Steps: 1000
- Number of Epochs: 3
- Weight Decay: 0.001
- Dropout Rate: 0.1
---
- Seed: 42


