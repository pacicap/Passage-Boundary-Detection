# Hyperparameters
LEARNING_RATE = 5e-5  # 1.5e-5, 2e-5, 3e-5, 5e-5
BATCH_SIZE = 8 # 8, 16, 32
WARMUP_STEPS = 600 # 0, 100, 1000, 10000
NUM_EPOCHS = 3 # 3, 5, 10
WEIGHT_DECAY = 1e-3  # 1e-2 or 1e-3
DROP_OUT_RATE = 0.1  # 0.1 or 0.2



Class distribution:
- continue: 3894
- not_continue: 3661
continue (after trim) : 1947

Training Epoch 1/3: 100% 561/561 [01:22<00:00,  6.81it/s]
Validating: 100% 71/71 [00:05<00:00, 12.31it/s]
* Training Loss: 0.6913, Validation Loss: 0.6445

Training Epoch 2/3: 100% 561/561 [01:25<00:00,  6.56it/s]
Validating: 100% 71/71 [00:05<00:00, 11.97it/s]
* Training Loss: 0.6399, Validation Loss: 0.7282

Training Epoch 3/3: 100% 561/561 [01:25<00:00,  6.55it/s]
Validating: 100% 71/71 [00:05<00:00, 11.95it/s]
* Training Loss: 0.4623, Validation Loss: 0.6815

Accuracy: 0.6369578134284016
Training Class-wise metrics:
continue: Precision = 0.48, Recall = 0.53, F1 = 0.51
not_continue: Precision = 0.74, Recall = 0.69, F1 = 0.71
Testing: 100% 71/71 [00:07<00:00,  9.79it/s]
Average test loss: 0.7328373399418844

Model: BERT

- Accuracy: 0.633
- Precision: 0.609
- Recall: 0.617
- F1 Score: 0.610
- AUC-ROC: 0.665
- Matthews Correlation Coefficient (MCC): 0.226
- Confusion Matrix:
              continue  not_continue
continue           110            85
not_continue       121           245

continue: Precision = 0.48, Recall = 0.53, F1 = 0.51
not_continue: Precision = 0.74, Recall = 0.69, F1 = 0.71

Hyperparameters:
- Learning Rate: 5e-05
- Batch Size: 8
- Warmup Steps: 600
- Number of Epochs: 3
- Weight Decay: 0.001
- Dropout Rate: 0.1
---
- Seed: 42